# Week 1 Notes

## "Terraform Beginner Bootcamp Week-1" video

### Create a S3 Bucket with Static Website 

1. On AWS console, go to S3

2. Create a bucket with a unique name and tag the bucket with your app.exampro.co UUID. 

3. Create the bucket. Go to the bucket.

4. Go to properties tab and scroll down to Static Website hosting and click enable.

5. Enter in index.html and error.html where directed. Save changes.

6. On the project's last branch (17) in GitHub, launch gitpod.

7. Make a new folder called public. In that folder create index.html

8. Go to ChatGPT and ask for an index.html for a reciepe. Mine was chocolate cake. Paste that into index.html in Gitpod and save.

9. In Gitpod terminal install a http server
`npm install http-sever -g`

10. Run the server and click on the link it gives you to see it
`http-server`

11. In Gitpod, go to the AWS terminal and list out your buckets
`aws s3 ls`
Find the bucket we created.

12. In the broswer find AWS S3 CLI and make sure you are on version 2.
> https://awscli.amazonaws.com/v2/documentation/api/latest/reference/s3/index.html
Scroll down and click cp (copy) 

13. Look for copy a local file to S3
> aws s3 cp test.txt s3://mybucket/test2.txt
edit this to include the file and bucket name
> aws s3 cp public/index.html s3://terraform-bootcamp-bucket-jas/index.html

14. In AWS S3 console, there should be link for your static website. It will not work as the bucket is not public and doesn't have a bucket policy.

### Use CloudFront to Serve our Static Website

1. In AWS console, go to cloudfront and create a distribution.

2. For origin domain, use the static website in the dropdown.

3. Click No on WAF.

4. In default root object, type index.html

5. In description add
> terrahouse example cdn

5. Click create distribution. Wait until it's enabled.

6. Click on the distribution and click the link it gives you. It doesn't work as we don't have a bucket policy

### Create Origin Access Control (OAC)

1. In AWS console in Cloudfront, in the left pane go to origin access. Click create control setting.

2. For name and description put
> terrahouse-example
Leave defaults and create

3. We can not add the OAC to the distribution because we should have added it when we were created and not use the static website endpoint as origin but the bucket as origin

4. Delete that distribution and create another one. Choose the bucket as the origin.

5. For origin access choose Origin access control settings (recommended). Choose the OAC we created. Make sure to the same settings as the last distribution. Click create distribution.

### Add Bucket Policy

1. After you created that new distribution, you will have a yellow pop-up saying you need to add a bucket policy. Click to copy the policy then click on the link it gives.

2. Click edit under bucket policy and paste the policy in there. Click save changes.

3. In AWS console for Cloudfront, click the link once it is up and it works.

## "Journal TOC and Major Version" video

### Create issue and Branch

1. In Github project repo, create an issue with the name
> Create TOC Readme
In the comments add
> - [ ] Create table of contents in our README
Create the issue.
> - [ ] Create journal folder

2. Create a branch off of the issue.

3. Go to Code tab, choose the newly created branch and click Gitpod

### Edit README for TOC

1. In Github, create a new file named /journal/.keep then create /journal/week0.md.

2. Copy the README.md and add it to journal/week0

3. Go to GitHub Wiki TOC generator
paste the READNE.md and copy the TOC that is generated.
> https://ecotrust-canada.github.io/markdown-toc/

```markdown
- [Terraform Beginner Bootcamp 2023](#terraform-beginner-bootcamp-2023)
  * [Semantic Versioning](#semantic-versioning)
  * [Install the Terraform CLI](#install-the-terraform-cli)
    + [Considerations with the Terraform CLI changes](#considerations-with-the-terraform-cli-changes)
    + [Considerations for Linux Distribution](#considerations-for-linux-distribution)
    + [Refactoring into Bash Scripts](#refactoring-into-bash-scripts)
      - [Shebang Considerations](#shebang-considerations)
      - [Execution Considerations](#execution-considerations)
      - [Linux Permissions Considerations](#linux-permissions-considerations)
    + [GitHub Lifecycle: Before, Init, Command](#github-lifecycle--before--init--command)
    + [Working with Env Vars](#working-with-env-vars)
      - [Setting and Unsetting Env Vars](#setting-and-unsetting-env-vars)
      - [Printing Vars](#printing-vars)
      - [Scoping for Env Vars](#scoping-for-env-vars)
      - [Persisting Env Vars in Gitpod](#persisting-env-vars-in-gitpod)
    + [AWS CLI Installation](#aws-cli-installation)
  * [Terraform Basics](#terraform-basics)
    + [Terraform Registry](#terraform-registry)
    + [Terraform Console](#terraform-console)
      - [Terrafrom init](#terrafrom-init)
      - [Terraform Plan](#terraform-plan)
      - [Terraform Apply](#terraform-apply)
        * [Problems Applying S3 Bucket](#problems-applying-s3-bucket)
      - [Terraform Destroy](#terraform-destroy)
      - [Terraform Lock files](#terraform-lock-files)
      - [Terraform State files](#terraform-state-files)
      - [Terraform Directory](#terraform-directory)
  * [Issues with Terraform Cloud Login and Gitpod Workspace](#issues-with-terraform-cloud-login-and-gitpod-workspace)
  * [Add your AWS credentials to Terraform Cloud](#add-your-aws-credentials-to-terraform-cloud)
```

4. In README.md for branch 19 remove a lot of stuff to where only this is left.
For the screenshot of the architectural Diagram, screenshot week 1
> https://lucid.app/lucidchart/e3f15b1a-2211-4ddb-8c95-f144c2504db4/edit?invitationId=inv_0873b3c6-c652-463f-9f2b-fa0f1b420823&page=0_0#

```markdown
# Terraform Beginner Bootcamp 2023

<img width="1373" alt="Architectual-Diagram" src="https://github.com/jasmine-rios/terraform-beginner-bootcamp-2023/assets/93607592/91b42e37-1acf-4ba6-9ee0-8cf9daf51e4d">

## Weekly Journals
- [Week 0 Journal](/journal/week0.md)

## Extras
- [Github Markdown TOC Generator](https://ecotrust-canada.github.io/markdown-toc/)

```
Commit the changes.

5. Make changes to README.md at

`### Gitpod Lifecyle (Before, Init, Commmand)`
to
`## Gitpod Lifecyle (Before, Init, Commmand)`

`### Working with Env Vars`
to
`## Working with Env Vars`

`#### Setting and Unsetting Env Vars`
to 
`### Setting and Unsetting Env Vars`

`#### Printing Vars`
to
`### Printing Vars`

`#### Scoping for Env Vars`
to
`### Scoping for Env Vars`

`#### Persisting Env Vars in Gitpod`
to
`### Persisting Env Vars in Gitpod`

`### AWS CLI Installation`
to
`## AWS CLI Installation`

6. Copy the body with the changes and add it to the TOC generator then add the new TOC to week0.md

7. Create week1 file in journal. Copy week0 first line and add it to the top of new file
`# Terraform Beginner Bootcamp 2023 - Week1`
commit it

8. In README.md add a link to the new week1 journal file.
`- [Week 1 Journal](/journal/week1.md)`

### Create a PR

1. In Github issues check off the tasks.

2. In Github Pull requests create a PR and paste the issues as the description.

3. Squash and merge

4. Add tags to main branch using gitpod but we are using each week as a major so it will be
`git tag 1.0.0`

## "Restructure Root Module" video

### Create an issue
1. Create a new issue in GitHub
> Restructure Root Module
With description
> - [] variables.tf
> - [] outputs.tf
> - [] main.tf
> - [] providers.tf
>
> https://developer.hashicorp.com/terraform/language/modules/develop/structure

2. Create a new branch off of the issue. Open Gitpod for the new branch

3. Edit week1.md and add this. The ascii was generated using ChatGPT

```markdown
## Root Module Structure

Our root module structure is as follows:
```
PROJECT_ROOT
    |
    |--- main.tf - everything else
    |
    |--- variables.tf - stores the structure of input variables
    |
    |--- terraform.tfvars - the data of variables we want to load into our terraform project
    |
    |--- providers.tf - defined required providers and their configuration
    |
    |--- outputs.tf - stores our outputs
    |
    |--- README.md - required for root modules
```

[Standard Module Structure](https://developer.hashicorp.com/terraform/language/modules/develop/structure)
```
### Add files and add content

1. Create the following files into the main project:
> variables.tf
> outputs.tf
> providers.tf
> terraform.tfvars

2. Split the screen between main.tf and terraform.tfvars
cut and paste into terraform.tfvars

```go
terraform {
    cloud {
    organization = "example-org-0dcec0"

    workspaces {
      name = "terra-house-hello-kitty-island-adventure"
    }
  }
  required_providers {
    random = {
      source = "hashicorp/random"
      version = "3.5.1"
    }
    aws = {
      source = "hashicorp/aws"
      version = "5.17.0"
    }
  }
}

provider "aws" {
  # Configuration options
}

provider "random" {
  # Configuration options
}
```

3. Split screen between main.tf and outputs.tf

```go
output "random_bucket_name" {
  value = random_string.bucket_name.result
}
```

4. Go to the Terraform registry and find the AWS documentation on S3 Bucket Tags

> https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/s3_bucket

Copy the tag portion in the first example and add it to the resource for aws_s3_bucket in main.tf. Modify the tags to show UserUuid

```go
resource "aws_s3_bucket" "example" {
  # Bucket Naming Rules
  # https://docs.aws.amazon.com/AmazonS3/latest/userguide/bucketnamingrules.html
  bucket = random_string.bucket_name.result

  tags = {
    UserUuid = var.user_uuid
  }
}

```
5. In variables.tf add what ChatGPT tells you when you ask to define a variable for UserUuid and validate it. Mine looks like this

```go
variable "user_uuid" {
  description = "User UUID"
  type        = string

  validation {
    condition     = regex("^[a-f0-9]{8}-[a-f0-9]{4}-[a-f0-9]{4}-[a-f0-9]{4}-[a-f0-9]{12}$", var.user_uuid)
    error_message = "Invalid User UUID format. It should be in the format xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx"
  }
}


```

6. In terraform cli do an init. 

If you get a failed to read "ExamPro" at host app.terraform.io and it says it's unauthorized, your token on Terraform Cloud is expired and you need to add a new one

7. Do a terraform plan. 

### Problem: I am getting an error of │ Error: Unexpected "terraform" block
```bash
│ 
│   on terraform.tfvars line 1:
│    1: terraform {
│ 
│ Blocks are not allowed here.
```

I am going to remove the terraform block and see if it fixes it. Now I am getting an unexpected block for cloud. Apparently .tfvars files are meant for variable values and don't support blocks

Solution: I had put the blocks for provider in terraform.tfvars but it needed to be in variables.tf. Now I am stuck on the same error we are in the video

8. Doing a terraform plan gives us this error

```bash
│ Error: No value for required variable
│ 
│   on variables.tf line 1:
│    1: variable "user_uuid" {
│ 
│ The root module input variable "user_uuid" is not set, and has no default
│ value. Use a -var or -var-file command line argument to provide a value for
│ this variable.
```
This is because we are using terraform cloud for our state files. It runs slower so we are going to switch to local for our state files and go back to Terraform Cloud later.

use this command to see if it the validation of the variable works
`terraform plan -var user_uuid='testing132'`

I was supposed to get invalid value for variable (shown at 18:34) but I got error in function cell
```bash
Error in function call
│ 
│   on variables.tf line 6, in variable "user_uuid":
│    6:     condition     = regex("^[a-f0-9]{8}-[a-f0-9]{4}-[a-f0-9]{4}-[a-f0-9]{4}-[a-f0-9]{12}$", var.user_uuid)
│     ├────────────────
│     │ while calling regex(pattern, string)
│     │ var.user_uuid is "testing132"
│ 
│ Call to function "regex" failed: pattern did not match any part of the
│ given string.
```
Hopefully that's okay 

9. Run a destroy terraform so we can switch to having the state file locally. It won't let us until we comment out everything in variables.tf and tags block in main.tf.

In the video, we were supposed to get an error because Terraform Cloud in the video doesn't have permissions to work with AWS resources. Mine must has permissions because I set my AWS credential variables in AWS

If yours didn't, you would go to Terraform Cloud and go to your workspace then the variables tab and set environment variables AWS_SECRET_ACCESS_KEY (sensitive), AWS_ACCESS_KEY_ID (sensitive), and AWS_DEFAULT_REGION

10. In provider.tf commit out the cloud block for time being and remove the main.tf comment on the S3 bucket tags

11. Do a terraform init because we got rid of the provider by commenting it out. You should get an error saying 

```bash
$ terraform init

Initializing the backend...
Migrating from Terraform Cloud to local state.
╷
│ Error: Migrating state from Terraform Cloud to another backend is not yet implemented.
│ 
│ Please use the API to do this: https://www.terraform.io/docs/cloud/api/state-versions.html
│ 
```

That's okay. We are going to delete our terraform.lock.hcl and delete the whole bin folder and contents

11. Now with that deleted, do a tf init and tf plan
Now it gives us an error of
```bash
$ terraform plan
╷
│ Error: Reference to undeclared input variable
│ 
│   on main.tf line 15, in resource "aws_s3_bucket" "example":
│   15:     UserUuid = var.user_uuid
│ 
│ An input variable with the name "user_uuid" has not been declared. This variable can be declared with a variable
│ "user_uuid" {} block.
```

This is because it is still expecting you to supply this variable.

12. Uncomment your variable.tf. Go to your ExamPro account and copy your userUuid and supply it to terminal
`terraform plan -var user_uuid="########-####-####-####-############"`

#### Problem: Command above gives Invalid validation result.
In the video (29:29) the command above works. Mine doesn't and gives an error like this

```bash
│ Error: Invalid variable validation result
│ 
│   on variables.tf line 6, in variable "user_uuid":
│    6:     condition     = regex("^[a-f0-9]{8}-[a-f0-9]{4}-[a-f0-9]{4}-[a-f0-9]{4}-[a-f0-9]{12}$", var.user_uuid)
│     ├────────────────
│     │ var.user_uuid is "d698d686-2c3c-4cd8-ab27-7ff00bc40152"
│ 
│ Invalid validation condition result value: a bool is required.
```
I am just going to go to the finished project repo and get the variables.tf as ChatGPT is always giving me code that doesn't work well and I believe it's the regex function not working well.

```go
variable "user_uuid" {
  description = "The UUID of the user"
  type        = string
  validation {
    condition        = can(regex("^[0-9a-fA-F]{8}-[0-9a-fA-F]{4}-[1-5][0-9a-fA-F]{3}-[89abAB][0-9a-fA-F]{3}-[0-9a-fA-F]{12}$", var.user_uuid))
    error_message    = "The user_uuid value is not a valid UUID."
  }
}
```
That fixes my issue

12. When specifying the user_uuid variable you can do that command or in the terrafor.tfvars file add the variable and run `terraform variable`

Now, add that user-uuid variable to Terraform Cloud Workspace Environment variables because we will use that when running the state file on Terraform Cloud

13. Create a file called terraform.tfvars.example and add the variable so we know what it looks like. (user_uuid is not sensitive so it doesn't matter if it shows publicly in repo)

14. Go to .gitpod.yml and add to the terraform before statement at the end. Also run it in terminal to make sure that it works by looking into terraform.tfvars
```yml
      cp $PROJECT_ROOT/terraform.tfvars.example $PROJECT_ROOT/terraform.tfvars
```

### Update documentation

1. Go to week1.md and add
```markdown
## Terraform and Input Variables
### Terraform Cloud Variables

In terraform we can set two kinds of variables:
- Environment Variables - those you would set in your bash terminal eg. AWS credentials
- Terraform Variables - those you would normally set in your tfvars file

We can set Terraform Cloud variables to be sensentive so they are not visibily in the UI. 

### Loading Terraform Input Variables

[Terraform INput Variables](https://developer.hashicorp.com/terraform/language/values/variables)
### Var Flag

We can use the `-var` flag to set an input variable or override a variable in the tfvars file eg. `terraform -var user_uuid="my user_id"`

### Var-file Flag

TODO:

### terraform.tvfars

Instead of running the var flag, you can add variable to terraform.tfvars to make the variable specified and you should only have to run `terraform plan` without having to specify the `-var` flag.

### Order of Terraform Variables

TODO: document which terraform variables take precedence


```

**The TODOs are homework to add**

2. We are going to intentionally do a terraform apply to show what we do when the tfstate file gets exposed in next video.
`tf apply --auto-approve`

3. Stage and commit changes with 

> #21 change structure of root module and add tagging to bucket

### Create PR, merge add tags

1. Follow the procedure from the last videos
In the video, they found out they were doing work in an old branch.
To fix that
`git fetch`
`git checkout<new branch name>`
`git merge <old branch> <new branch>`

## "Terraform Import and Configuration Drift" video

### Terraform Input to get the terraform.tfstate after deletion

If you loose your state file and you have a lot of stuff on it, you might not be able to recover because you can't import on all resources.

You might be able to recover, if not a lot of resources it could be possible to recover. **ALWAYS STORE STATE FILE INTO SOMETHING NOT LOCAL E.G TERRAFORM CLOUD**

1. Launch a Gitpod from main branch w/o creating a branch.

2. In a browser, navigate to Terraform's documentation about import
> https://developer.hashicorp.com/terraform/language/import
We are going to use import command but we could make an import.tf

3. Go to AWS console for terraform_begineer_bootcamp user. Go to S3. You see a bucket in there but it's no longer managed by terraform because we lost our state file.

4. In Gitpod terminal, 
`tf init`

5. Go to Terraform Registry AWS documentation and go to S3 bucket. Find the Import on the right pane.
Use the import command in Gitpod terminal
`terraform import aws_s3_bucket.example 1wakpj698imyssoncxa1hxx9bx74xknf`

6. Now we will need to import random_string. Go to the Terraform Registry for Random and find the random-string and see the import command for it.
> https://registry.terraform.io/providers/hashicorp/random/latest/docs/resources/string#import

`import random_string.bucket_name 1wakpj698imyssoncxa1hxx9bx74xknf`

7. Run a terraform plan to see if it shows the import impact. It doesn't at bucket name so we need to find a better solution that Random

```bash
  # aws_s3_bucket.example must be replaced
-/+ resource "aws_s3_bucket" "example" {
      + acceleration_status         = (known after apply)
      + acl                         = (known after apply)
      ~ arn                         = "arn:aws:s3:::1wakpj698imyssoncxa1hxx9bx74xknf" -> (known after apply)
      ~ bucket                      = "1wakpj698imyssoncxa1hxx9bx74xknf" # forces replacement -> (known after apply) # forces replacement
```
8. Take out random from providers.tf. In main.tf take out random string and replace "random_string" in resource aws_s3_bucket with "var".

```go
resource "aws_s3_bucket" "example" {
  # Bucket Naming Rules
  # https://docs.aws.amazon.com/AmazonS3/latest/userguide/bucketnamingrules.html
  bucket = var.bucket_name
  tags = {
    UserUuid = var.user_uuid
  }
}
```
9. Go to terraform.tfvars.example and add bucket_name var as the bucket name you already have
`bucket_name="1wakpj698imyssoncxa1hxx9bx74xknf"`
Copy that to terraform.tfvars

10. Clear and then do `tf plan`. It should fail with this error
```bash
$ tf plan
╷
│ Warning: Value for undeclared variable
│ 
│ The root module does not declare a variable named "bucket_name" but a value
│ was found in file "terraform.tfvars". If you meant to use this value, add a
│ "variable" block to the configuration.
│ 
│ To silence these warnings, use TF_VAR_... environment variables to provide
│ certain "global" settings to all configurations in your organization. To
│ reduce the verbosity of these warnings, use the -compact-warnings option.
╵
╷
│ Error: Reference to undeclared input variable
│ 
│   on main.tf line 5, in resource "aws_s3_bucket" "example":
│    5:   bucket = var.bucket_name
│ 
│ An input variable with the name "bucket_name" has not been declared. This
│ variable can be declared with a variable "bucket_name" {} block.
╵
╷
│ Error: Reference to undeclared resource
│ 
│   on outputs.tf line 2, in output "random_bucket_name":
│    2:   value = random_string.bucket_name.result
│ 
│ A managed resource "random_string" "bucket_name" has not been declared in the
│ root module.
╵
```

11. In variables.tf add bucket_name {} block using ChatGPT by asking it write terraform variable for bucket_name, validate the string so its a valid bucket name for AWS.

ChatGPT once again gave me code with a lot of errors so I copied the one in the finished project
```go
variable "bucket_name" {
 description = "The name of the S3 bucket"
 type        = string

 validation {
   condition     = (
     length(var.bucket_name) >= 3 && length(var.bucket_name) <= 63 && 
     can(regex("^[a-z0-9][a-z0-9-.]*[a-z0-9]$", var.bucket_name))
   )
   error_message = "The bucket name must be between 3 and 63 characters, start and end with a lowercase letter or number, and can contain only lowercase letters, numbers, hyphens, and dots."
 }
}

```

12. In outputs change thr output and value
```go


```
13. In main.tf change resource "aws_S3_bucket" "example" to "website_bucket"
`resource "aws_s3_bucket" "website_bucket" {`

14. Run a tf plan and it should make it where the bucket name on AWS doesn't change.

15. Run `tf destroy` then `tf apply --auto-approve`

### Add documentation

1. In week1.md add this
```markdown
## Dealing with Configuration Drift

## What happens if we lose our state file?

If you lose your statefile, you most likely have to tear down all your cloud infrastructure manually. 

You can use terraform import but it won't work for all cloud resources. You need to check the terraform providers documentation for which resources support import.

### Fix Missing Resources with Terraform Import

`terraform import aws_s3_bucket.website_bucket`

[Terraform Import](https://developer.hashicorp.com/terraform/language/import)
[AWS S3 Bucket Import](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/s3_bucket#import)
### Fix Manual Configuration

If someone goes and deletes or modifies cloud resources manually through ClickOps.

If we run Terraform plan with attempt to put our infrastructure back into the expected state fixing Configuration Drift
```
2. Create an issue with name

> Configuration Drift

Description

> - [ ] use terraform import
> - [ ] purposely cause configuration drift via clickops, and correct state.

3. Check off issue's tasks. Create branch from issue.

4. We don't want to do a commit in main so 
`git fetch`
`git checkout`

3. Stage and commit with message
> #23 bucket should no longer use Random provider

4. Create PR and squash and merge.

5. Add tags to main branch in Gitpod then shut down

## "Create Terrahouse Module" video

### Create nested modules

1. Create an issue in Github name it
> AWS Terrahouse Module
Description
> - [ ] Setup directory structure for our module
> - [ ] Port our S3 bucket into the module

2. Create a branch off the issue and open Gitpod

3. Create a new folder called
> modules

4. Under the modules directory create a folder
> terrahouse_aws

5. Under the directory terrahouse_aws create
> main.tf
> variables.tf
> outputs.tf
> README.md
> License 

6. In browser search for Anton Babenko terraform_aws_modules. 
> https://github.com/terraform-aws-modules/terraform-aws-vpc

7. Go to the license file and copy the whole file contents and paste into license file

8. Go to our main.tf and cut the content out and paste it into main.tf under terrahouse_aws

9. Take the required parameters and aws provider from providers.tf and paste them into main.tf under terrahouse_aws at the top. Add terraform {} before it

10. In providers.tf cut out all the content and paste into main.tf. Delete providers.tf. 

**NOTE** Andrew for some reason has more in his providers.tf than me. I am going to ignore it for now but if I need to add it in (08:27) is where it is at.

11. In variables.tf cut the contents and paste into variables.tf in terrahouse_aws

12. In ouputs.tf cut and paste into outputs.tf under terrahouse_aws.
13. In main.tf we need to set it up to pass through modules
```go
modules "terrahouse_aws" {
    source = "./modules/terrahouse_aws"
    user_uuid = var.user_uuid
    bucket_name = var.bucket_name
}
```
14. Go to terraform module sources in web browser and copy/paste into journal/week1.md

> https://developer.hashicorp.com/terraform/language/modules/sources

Write some more into week1.md

```markdown
## Terraform Modules

### Terraform Module Structure

It is recommended to place modules in a `modules` directory when locally developing modules but you can name it whatever you like.

### Passing Input Variables

We can pass input variables to our module.
The module has to declare these terrform variables in its own variables.tf

```go
module "terrahouse_aws" {
    source = "./modules/terrahouse_aws"
    user_uuid = var.user_uuid
    bucket_name = var.bucket_name
}
```

### Module Sources

Using the source we can import the module from various sources e.g.
- locally
- GitHub
- Terraform Registry

```go
modules "terrahouse_aws" {
    source = "./modules/terrahouse_aws"
}
```
(https://developer.hashicorp.com/terraform/language/modules/sources)


```
```

15. In Terraform terminal do a `tf init`. You will have to remove provider aws block from main.tf under terrahouse because it is empty

16. Do a `tf plan` to see if it works. We get errors about our variables list in main.tf. This is because we need to define them in the top level varaibles.tf. Add into variables.tf

```go
variable "user_uuid" {
type = string
}

variable "bucket_name" {
type = string
}
```

17. Run `tf plan` then `tf apply`

18. Run `tf output`. It should say none is found. We need to refrence our nested outputs.tf in our top-level outputs.tf.
In outputs.tf add

```go
output "bucket_name" {
  description = "Bucket name for our static website hosting"
  value = module.terrahouse_aws.bucket_name
}
```
19. In week1.md before `## Terraform Modules` add this from the webpage on the terraform command 'refresh'
> https://developer.hashicorp.com/terraform/cli/commands/refresh#usage

```markdown
## Fix using Terraform Refresh

```sh
terraform apply -refresh-only -auto-approve
```
```
```
20. Do a `tf plan` then `tf apply --auto-approve` then `tf outputs`. 
`tf destroy`

### Commit and PR 

1. Save and stage changes. Commit with 
> #25 refactor s3 bucket into the new terrahouse_aws module

2. Check off issues. Copy issues.

3. Create a PR and comment the issues. Squash and merge.

4. Add tag of 1.3.0 to main

5. Close Gitpod

## "Static Website Hosting" video

### Create an Issue and Launch GitPod

1. In GitHub create an issue named
> S3 Static Website Hosting
with a description of
> - [ ] Configure out bucket for S3 static website hosting
- [ ] Upload an index.html
- [ ] Upload an error.html
- [ ] Update our outputs for static website hosting url

2. Create a branch off of issue and launch branch in Gitpod

### ChatGPT and Terraform Registry

1. In chatGPT ask
> write terraform for static website hosting for an S3 bucket
Copy the code for the resource it gives and see if it works.
```go
resource "aws_s3_bucket" "static_website" {
  bucket = "<your-bucket-name>"
  acl    = "public-read" # Sets bucket ACL to allow public read access

  website {
    index_document = "index.html" # The default index document
    error_document = "error.html" # The error document
  }
}

```

2. Paste this into our main.tf under terrahouse_AWS at the end.

3. Change bucket name to aws_s3_bucket.website_bucket.bucket.

4. Do `tf init` and `tf plan`. tf plan should fail because agruement is depreciated. It gives us the wrong thing because it is trained on old AWS provider, not on AWS provider 5.0

5. Go to terraform registry and AWS provider documentation. Copy what it told you to use in terminal. Copy the code with routing
> https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/s3_bucket_website_configuration#example-usage

6. In main.tf replace the old code with the copied code
Rename "example" to "website_configuration"
Rename bucket to be aws_s3_bucket.website_bucket.bucket
Take out the routing rules part

7. Do `tf plan` then `tf apply --auto-approve`

8. Go to S3 and find that bucket and go to properties and scroll down to static website. We want to ouput this link. 
Go to outputs.tf in nested module. Add an output to "website endpoint"

```go
output "website_endpoint" {
  value = aws_s3_bucket_website_configuration.website_configuration.website_endpoint
}
```
9. In outputs.tf add a refrence to the nested file
```go
output "S3_website_endpoint" {
  description = "s3 website static hosting website"
  value = module.terrahouse_aws.website_endpoint
}
```
10. Do `tf plan` and then `tf apply --auto-approve`

### Update week1.md

1. In week1.md add 
```markdown
## Considerations when using ChatGPT to write Terraform

LLMs (Large Language Models) such as ChatGPT may not be trained about documentation or information about Terraform.

It may likely produce older examples that could be deprecated. Often affecting providers.
```
2. Go to ChatGPT and ask
> upload an index.html file to our S3 bucket using terraform code
Copy the code it gives you.

3. Go to main.tf nested and add to the last block
```go
resource "aws_s3_bucket_object" "index_html" {
  bucket = aws_s3_bucket.static_website.bucket.bucket
  key    = "index.html" # The key (filename) of the object in the bucket
  acl    = "public-read" # Set ACL to allow public read access
```

4. Go to Terraform AWS documentents add search for aws_s3_object. Copy the link.
> https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/s3_object

5. In main.tf nested add the link before your code and commit it out.
Replace the code from ChatGPT with the example usuage code.

6. Add the link from the website configuration to the top of the code in main.tf nested and comment it out

7. In nested main.tf change bucket name to aws_s3_bucket.website_bucket
and key to "index.html"

8. To find the path look up path terraform in google. Copy the link.
> https://developer.hashicorp.com/terraform/language/expressions/references#filesystem-and-workspace-info

9. In week1.md add
```markdown
## Working with Files in Terraform

In terraform there is a special variable in path that allows us to specify local paths.
- path.module = get the path for the current module
-path.root = get the path for the root module

[Special Path Variable](https://developer.hashicorp.com/terraform/language/expressions/references#filesystem-and-workspace-info)
```
10. Usually you can use `tf console` and ask for path.module or path.root but it is giving us '.' which is not it. 
Make a new directory named public and files index.hml and error.html

11. In index.html put `hello terraformers` and in error.html `Error found!`

12. Drag nested main.tf to terminal to get the full path. Paste into source with "" around it.
Comment out etag.

13. Do `tf plan` and `tf apply --auto-approve`

14. Go to S3 console and see index.html in bucket

### Make source path more modular

1. You don't want to make the path hardset to what we have because things change. 
Add source as this. Path is from dragging index.html to terminal
`source = "${path.root}/public/index.html"`

2. Do a `tf apply`. See in S3 console that file changed.

3. Add more exclamations to index.hml to see if it changes

4. Do `tf apply` It will say nothing will change because tf doesn't read data. We can know data changed by eTag.
Uncomment eTag in main.tf nested and change the path.

5. Do `tf apply`. We see our etag update.

### Make source path super modular by using Terraform Variable

1. We could refactor the path to be more modular for people so they can name it something other than index.html.
Ask ChatGPT
> Write a terraform variable declaration for index_html_filepath and ensure that its a valid path
Code the code it gives

2. In variables.tf nested and pasted the copied code at the end.

3. In top-level variables.tf add the variable

4. In terraform.tfvars.example paste in the variable and add the path for both index.html and error.html
`index_html_filepath="/workspace/terraform-beginner-bootcamp-2023/public/index.html"`
`error_html_filepath="/workspace/terraform-beginner-bootcamp-2023/public/error.html"`

Add the lines also into terraform.tfvars

5. Change source and etag of index.html in nested main.tf to say var.index_html_filepath. Copy when done and paste below it and change index to error.

6. Change the names on the resource object lines for both blocksnested main.tf to say "error_html" and "index_html.

7. In nested variables.tf add for the error and do the same for top-level variables.tf

7. In main.tf add to source
`error_html_filepath = var.error_html_filepath`
`index_html_filepath = var.index_html_filepath`

8. Do `tf apply`. You get an error of 
```sh
│ Error: reading S3 Object (index.html): couldn't find resource
│ 
│   with module.terrahouse_aws.aws_s3_object.index_html,
│   on modules/terrahouse_aws/main.tf line 36, in resource "aws_s3_object" "index_html":
│   36: resource "aws_s3_object" "index_html" {
```

BUT when doing tf apply again. It works.

### Add to week1.md

1. In week1.md add
```markdown
## Working with Files in Terraform

### Fileexists function

This is a built-in terraform function that checks the exists of a file.
We used it in our variables.tf nested
```go
 validation {
    condition     = fileexists(var.error_html_filepath)
    error_message = "The specified file path for error.html does not exist."
  }
```
[Fileexists](https://developer.hashicorp.com/terraform/language/functions/fileexists)

### Filemd5

This is a built in function as well that makes a hash when you give it a path. We used it in helping to create a unique eTag each time we change data in that file we provided the path for.

[Filemd5](https://developer.hashicorp.com/terraform/language/functions/filemd5)

### Path Variables

In terraform there is a special variable in path that allows us to specify local paths.
- path.module = get the path for the current module
-path.root = get the path for the root module

[Special Path Variable](https://developer.hashicorp.com/terraform/language/expressions/references#filesystem-and-workspace-info)

resource "aws_s3_object" "index_html" {
  bucket = aws_s3_bucket.website_bucket.bucket
  key    = "index.html"
  source = "${path.root}/public/index.html
  etag = filemd5(var.index_html_filepath)
}
```
```

### Save, Stage, Commit, PR, tag

1. Do `tf destroy` 

2. Save, stage, and commit changes with
> #27 enabled static website hosting, uploaded index and error file via terraform

3. Check issues. Copy issues.

4. Create PR. Paste in issues as comment. Squash in Merge.

5. Tag main with 1.4.0 
We have a tag wrong. Go to Week1.md and add this
```markdown
## Fix tags 

Locally delete a tag
```sh
git tag -d <tag_name>
```

Remotely delete tag
```sh
git push --delete origin <tagname>
```

Checkout the commit that you want to retag. Gave the sha from your Github history

```sh
git checkout <SHA>
git tag M.m.P
git push --tags
git checkout main
```

[How to Delete Local and Remote Tags](https://devconnected.com/how-to-delete-local-and-remote-tags-on-git/)

```
```

6. Checkout to your messed up branch by going to Github and clicking commits and copy the sha.
Go to gitpod terminal and do `git checkout <SHA>`

7. It tells us to stash the changes we made to week1.md
`git add .`
`git stash save`
`git checkout <SHA>`
then if you dont have the tag add it and push the tag

8. `git checkout main`
`git stash apply`
Save changes to week1.md and stage them. Commit with
> add fixing tags

9. Sync them then in terminal 
`git tag 1.4.1`
Close Gitpod.

## "Content Delivery Network" video

### Ask ChatGPT vs Terraform AWS Documentation

1. Go to ChatCPT and ask
> AWS CloudFront serving static website hosting for an S3 bucket
It will give outdated info as OAI (Origin Access Identity) is now OAC (Origin Access Controls)

2. In browser look up origin access controls aws cloudfront and see that it was implemented last year and that's why ChatGPT is wrong sometimes. Also, it doesn't rely on best practices so that's why we need to rely on Tf docs.
> https://aws.amazon.com/blogs/networking-and-content-delivery/amazon-cloudfront-introduces-origin-access-control-oac/

3. In Terraform registry for AWS go to the documentation and search 
> aws_cloudfront_distribution
Click it in the resource tab on the left panel
The first code is long so we will break up the data in the nested modules.

### Create a issue

1. In GitHub create a issue named
> CDN Implementation
with description of
> - [ ] CloudFront Distribution
> - [ ] CloudFront Origin Access Control
> - [ ] Bucket Policy

2. Create a branch off of issue and launch Gitpod for that branch

### Split up CDN and Storage

1. Create two new files under terrahouse_aws
- resource-cdn.tf
- resource-storage.tf

2. In nested main.tf grab everything but the providers block and cut it out
Paste into resource-storage.tf

3. In the webpage we had open eariler grab the code from bottom up to resouce CloudFront Distribution and copy it
> https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/cloudfront_distribution#example-usage

4. Go to resource-cdn.tf and paste the code you copied. 
This is the code:
```go
resource "aws_cloudfront_distribution" "s3_distribution" {
  origin {
    domain_name              = aws_s3_bucket.b.bucket_regional_domain_name
    origin_access_control_id = aws_cloudfront_origin_access_control.default.id
    origin_id                = local.s3_origin_id
  }

  enabled             = true
  is_ipv6_enabled     = true
  comment             = "Some comment"
  default_root_object = "index.html"

  logging_config {
    include_cookies = false
    bucket          = "mylogs.s3.amazonaws.com"
    prefix          = "myprefix"
  }

  aliases = ["mysite.example.com", "yoursite.example.com"]

  default_cache_behavior {
    allowed_methods  = ["DELETE", "GET", "HEAD", "OPTIONS", "PATCH", "POST", "PUT"]
    cached_methods   = ["GET", "HEAD"]
    target_origin_id = local.s3_origin_id

    forwarded_values {
      query_string = false

      cookies {
        forward = "none"
      }
    }

    viewer_protocol_policy = "allow-all"
    min_ttl                = 0
    default_ttl            = 3600
    max_ttl                = 86400
  }

  # Cache behavior with precedence 0
  ordered_cache_behavior {
    path_pattern     = "/content/immutable/*"
    allowed_methods  = ["GET", "HEAD", "OPTIONS"]
    cached_methods   = ["GET", "HEAD", "OPTIONS"]
    target_origin_id = local.s3_origin_id

    forwarded_values {
      query_string = false
      headers      = ["Origin"]

      cookies {
        forward = "none"
      }
    }

    min_ttl                = 0
    default_ttl            = 86400
    max_ttl                = 31536000
    compress               = true
    viewer_protocol_policy = "redirect-to-https"
  }

  # Cache behavior with precedence 1
  ordered_cache_behavior {
    path_pattern     = "/content/*"
    allowed_methods  = ["GET", "HEAD", "OPTIONS"]
    cached_methods   = ["GET", "HEAD"]
    target_origin_id = local.s3_origin_id

    forwarded_values {
      query_string = false

      cookies {
        forward = "none"
      }
    }

    min_ttl                = 0
    default_ttl            = 3600
    max_ttl                = 86400
    compress               = true
    viewer_protocol_policy = "redirect-to-https"
  }

  price_class = "PriceClass_200"

  restrictions {
    geo_restriction {
      restriction_type = "whitelist"
      locations        = ["US", "CA", "GB", "DE"]
    }
  }

  tags = {
    Environment = "production"
  }

  viewer_certificate {
    cloudfront_default_certificate = true
  }
}
```

5. Copy the URL you just got the code from and paste above the code and comment it out.

6. Under the code for orgin in the domain portion change the = to aws_s3_bucket.website_bucket.bucket_regional_domain_name

7. Change comment to = "Static website hosting for: ${var.bucket_name}

8. Remove logging_config block because we don't want logging

9. Comment out alias.

10. Delete all the caching blocks for # Cache behavior with precedence 0 and 1

11. For restrictions- geo restrictions change "whitelist" to "none" and take out locations to where it is []

12. For tags remove environment and put UserUuid = var.user_uuid

13. On the top add 
```go
locals {
  s3_origin_id = "MyS3Origin"
}
```

14. In Terraform AWS documentation look up 
> aws_cloudfront_origin_access_control
> https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/cloudfront_origin_access_control
Copy the code and paste it in the front of resource_cdn.tf then in front of the code paste the link and comment it out. Change name to "default".

15. In the new code change name to = "OAC ${var.bucket_name}

16. Change description to "Origin Access Controls for Static Website Hosting ${var.bucket_name}

### Add Bucket Policy

1. Search in Terraform AWS for aws_s3_bucket_policy
> https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/s3_bucket_policy#example-usage
Find the code and copy it 
```go
resource "aws_s3_bucket_policy" "allow_access_from_another_account" {
  bucket = aws_s3_bucket.example.id
  policy = data.aws_iam_policy_document.allow_access_from_another_account.json
}

```
2. Paste into resource-storage.tf. Change the resource name to "bucket_policy"
Change bucket to aws_s3_bucket.website_bucket.bucket_name
Change Policy to jsonencode({}) with this you code in JSON right there

3. Copy the bucket policy from step 5 in AWS OAC website
> https://aws.amazon.com/blogs/networking-and-content-delivery/amazon-cloudfront-introduces-origin-access-control-oac/

4. Find the website on Terraform Data Sources
> https://developer.hashicorp.com/terraform/language/data-sources
Look for aws_caller_identity and this will have the IDENTITY we need in the Bucket Policy in resource-storage.tf

5. In nested main.tf paste in the link for data sources then comment it out.
> https://developer.hashicorp.com/terraform/language/data-sources
Add in the top line of the code under that link.

6. Now that we have that in our main.tf nested, we can interporlate that in our resource.storage.tf to get our IDENTITY
Now our resource-storage.tf will look like this

```go
# https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/s3_bucket
resource "aws_s3_bucket" "website_bucket" {
  # Bucket Naming Rules
  # https://docs.aws.amazon.com/AmazonS3/latest/userguide/bucketnamingrules.html
  bucket = var.bucket_name
  tags = {
    UserUuid = var.user_uuid
  }
}

# https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/s3_bucket_website_configuration
resource "aws_s3_bucket_website_configuration" "website_configuration" {
  bucket = aws_s3_bucket.website_bucket.bucket

  index_document {
    suffix = "index.html"
  }

  error_document {
    key = "error.html"
  }

}

# https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/s3_object
resource "aws_s3_object" "index_html" {
  bucket = aws_s3_bucket.website_bucket.bucket
  key    = "index.html"
  source = var.index_html_filepath
  etag = filemd5(var.index_html_filepath)
}
# https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/s3_object
resource "aws_s3_object" "error_html" {
  bucket = aws_s3_bucket.website_bucket.bucket
  key    = "error.html"
  source = var.error_html_filepath
  etag = filemd5(var.error_html_filepath)
}

resource "aws_s3_bucket_policy" "bucket_policy" {
  bucket = aws_s3_bucket.website_bucket.bucket
  policy = jsonencode({
    "Version" = "2012-10-17",
    "Statement" = [
        {
            "Sid" = "AllowCloudFrontServicePrincipalReadOnly",
            "Effect" = "Allow",
            "Principal" = {
                "Service" = "cloudfront.amazonaws.com"
            },
            "Action" = "s3:GetObject",
            "Resource" = "arn:aws:s3:::${aws_s3_bucket.website_bucket.id}/*",
            "Condition" = {
                "StringEquals" = {
                    "AWS:SourceArn" = "arn:aws:cloudfront::${data.aws_caller_identity.current.account_id}:distribution/DISTRIBUTION_ID"
                }
            }
        },
        
    ]
}
)
}


```
7. In our resource-storage.tf change the DISTRIBUTION_ID to what is in resource-cdn.tf which is ${aws_cloudfront_distribution.s3_distributionid}

8. Search up Terraform Locals for our documentation
> https://developer.hashicorp.com/terraform/language/values/locals
9. Do a `tf init` then `tf plan`. We should have 7 changes to add.

10. Do a `tf apply --auto-approve` and this should take a long time

11. Once it's done, go to your AWS console and look at CloudFront. Copy the distribution name in a browser and search it.
When we go to it, it downloads a file because we didn't specify the content type.

12. In resource_storage.tf and add the content type in aws_s3_object index.html and for error.html as well
`content_type = "text/html`

13. Do `tf apply` to see if it picks up the content_type. It does take the changes.

14. Refresh the AWS cloudfront page.Try the distribution link again, we get "Hello Terraformers!!"
In the video it didn't work for him and he added a html5 header he got from ChatGPT.

16. Go to jsonencode on terraform website so we can reference it in our documentation

15. Go to week1.md and add 
```markdown

## Terraform Locals
```go
locals {
  s3_origin_id = "myS3Origin"
}
```

[Locals Values](https://developer.hashicorp.com/terraform/language/values/locals)
## Terraform Data Sources

This allows us to source data from cloud resources.

This is useful when want to reference cloud resources without inputting them. e.g.

```go
data "aws_caller_identity" "current" {}
```


[Data Sources](https://developer.hashicorp.com/terraform/language/data-sources)

## Working with JSON

We use the jsonencode to create the json policy inline in the hcl.

```go
jsonencode({"hello"="world"})
{"hello":"world"}
```

[Jsonencode](https://developer.hashicorp.com/terraform/language/functions/jsonencode)

```
```

### Stage, commit, PR, and Tag

1. Stage all the changes and commit with 
> #29 create cdn and serve s3 bucket from cdn

2. Check issues. Copy issue description.

3. Create PR add issue to comment. Squash and merge

4. In Gitpod checkout to main and add 1.5.0 tag. Push the tags.

## "Terraform Data and Content Version" video

### Create Website Versions

We want to have multiple versions of our website instead of everytime we change our html files. Doing an invalidation with "/*" is an expensive call and we have to now that our website gets cached to servers all over the world too

### Create Issue

1. Create a GitHub issue named
> Setup Content Version
with a description of
> - [ ] only change files when we set a content version

2. Create a branch off the issue.

3. Launch Gitpod off of the new branch

### Make better pages for index.html and error.html

1. Ask chatGPT to make an HTML5 for index.html and copy and paste into index.html and edit it to say Welcome Terraformers!!!
```html
<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Hello Terraformers!!!</title>
</head>
<body>
    <header>
        <h1>Hello Terraformers!!!</h1>
    </header>
</body>
</html>

```
2. Do the same for error.html but put Error Found!

### Create Versions for our Website

1. In terraform.tfvars.example and terraform.tfvars add
`content_version=1`

2. Ask ChatGPT
> Write a terraform variable for contect_version that ensures it only uses positive integers starting at 1
Copy the code 
```go
variable "content_version" {
  description = "The content version. Should be a positive integer starting"
  type = number

  validation {
    condition = var.content_version > 0 && floor(var.content_version) == ßvar.content_version
    error_message = "Content version must be a positive integer starting at 1."
  }
}

```

3. In variables.tf nested, paste the code

4. In variables.tf add
```go
variable "content_version" {
  type = number
}
```

5. In a web broswer search for terraform resource lifecycle
> https://developer.hashicorp.com/terraform/language/meta-arguments/lifecycle
Copy the link and paste it in week1.md

```markdown
### Changing the Lifecycle of Resources

[Meta Agruements Lifecycle ](https://developer.hashicorp.com/terraform/language/meta-arguments/lifecycle)
```

6. Go to resource-storage.tf 
under resource for index.html and error.html add
```go
lifecycle {
    ignore_changes = [ etag ]
  }
```
Comment it out to observe it first

7. In main.tf add
`content_version = 1`

8. Do `tf init` then `tf apply` (Mine said OAC and S3 bucket already exists)

9. Change index.html a little bit then do a `tf plan`

10. Uncomment in resource-storage.tf the lifecycle block and run `tf plan`

11. Now we need to add content_version but we need a resource but we can uses terraform_data to do that at the end of resource-storage.tf
```go
resource "terraform_data" "content_version" {
  input = var.content_version
}
```
12. Now go to the lifecyle of index.html and add 
`replace_triggered_by = [terraform_data.content_version.output]`

13. Do `tf plan` then `tf apply`.

**PROBLEM**
I get this
```sh
│ Error: creating Amazon CloudFront Origin Access Control (OAC 1wakpj698imyssoncxa1hxx9bx74xknf): OriginAccessControlAlreadyExists: An origin access control with the same name already exists.
│       status code: 409, request id: 09d1943b-23da-4720-a312-4002b7f23f60
│ 
│   with module.terrahouse_aws.aws_cloudfront_origin_access_control.default,
│   on modules/terrahouse_aws/resource-cdn.tf line 3, in resource "aws_cloudfront_origin_access_control" "default":
│    3: resource "aws_cloudfront_origin_access_control" "default" {
│ 
```
Hopefully its okay

14. Change index.html and run `tf plan` to see if it the index.html. It shouldn't because we haven't triggered a content version.
**PROBLEM** Mine shows this so I am not sure if this is bad but I don't see anything about index.html

```sh
$ tf plan

module.terrahouse_aws.terraform_data.content_version: Refreshing state... [id=12f8fd3c-8d8b-9b6b-8855-5ed455af322f]
module.terrahouse_aws.data.aws_caller_identity.current: Reading...
module.terrahouse_aws.aws_s3_bucket.website_bucket: Refreshing state... [id=1wakpj698imyssoncxa1hxx9bx74xknf]
module.terrahouse_aws.data.aws_caller_identity.current: Read complete after 0s [id=816073131308]
module.terrahouse_aws.aws_s3_bucket_website_configuration.website_configuration: Refreshing state... [id=1wakpj698imyssoncxa1hxx9bx74xknf]
module.terrahouse_aws.aws_s3_object.error_html: Refreshing state... [id=error.html]
module.terrahouse_aws.aws_s3_object.index_html: Refreshing state... [id=index.html]

Terraform used the selected providers to generate the following execution plan.
Resource actions are indicated with the following symbols:
  + create

Terraform will perform the following actions:

  # module.terrahouse_aws.aws_cloudfront_distribution.s3_distribution will be created
  + resource "aws_cloudfront_distribution" "s3_distribution" {
      + arn                            = (known after apply)
      + caller_reference               = (known after apply)
      + comment                        = "Static websitre hosting for 1wakpj698imyssoncxa1hxx9bx74xknf"
      + default_root_object            = "index.html"
      + domain_name                    = (known after apply)
      + enabled                        = true
      + etag                           = (known after apply)
      + hosted_zone_id                 = (known after apply)
      + http_version                   = "http2"
      + id                             = (known after apply)
      + in_progress_validation_batches = (known after apply)
      + is_ipv6_enabled                = true
      + last_modified_time             = (known after apply)
      + price_class                    = "PriceClass_200"
      + retain_on_delete               = false
      + staging                        = false
      + status                         = (known after apply)
      + tags                           = {
          + "UserUuid" = "d698d686-2c3c-4cd8-ab27-7ff00bc40152"
        }
      + tags_all                       = {
          + "UserUuid" = "d698d686-2c3c-4cd8-ab27-7ff00bc40152"
        }
      + trusted_key_groups             = (known after apply)
      + trusted_signers                = (known after apply)
      + wait_for_deployment            = true

      + default_cache_behavior {
          + allowed_methods        = [
              + "DELETE",
              + "GET",
              + "HEAD",
              + "OPTIONS",
              + "PATCH",
              + "POST",
              + "PUT",
            ]
          + cached_methods         = [
              + "GET",
              + "HEAD",
            ]
          + compress               = false
          + default_ttl            = 3600
          + max_ttl                = 86400
          + min_ttl                = 0
          + target_origin_id       = "myS3Origin"
          + trusted_key_groups     = (known after apply)
          + trusted_signers        = (known after apply)
          + viewer_protocol_policy = "allow-all"

          + forwarded_values {
              + headers                 = (known after apply)
              + query_string            = false
              + query_string_cache_keys = (known after apply)

              + cookies {
                  + forward           = "none"
                  + whitelisted_names = (known after apply)
                }
            }
        }

      + origin {
          + connection_attempts      = 3
          + connection_timeout       = 10
          + domain_name              = "1wakpj698imyssoncxa1hxx9bx74xknf.s3.us-east-1.amazonaws.com"
          + origin_access_control_id = (known after apply)
          + origin_id                = "myS3Origin"
        }

      + restrictions {
          + geo_restriction {
              + locations        = (known after apply)
              + restriction_type = "none"
            }
        }

      + viewer_certificate {
          + cloudfront_default_certificate = true
          + minimum_protocol_version       = "TLSv1"
        }
    }

  # module.terrahouse_aws.aws_cloudfront_origin_access_control.default will be created
  + resource "aws_cloudfront_origin_access_control" "default" {
      + description                       = "Origin Access Controls for Static Website Hosting 1wakpj698imyssoncxa1hxx9bx74xknf"
      + etag                              = (known after apply)
      + id                                = (known after apply)
      + name                              = "OAC 1wakpj698imyssoncxa1hxx9bx74xknf"
      + origin_access_control_origin_type = "s3"
      + signing_behavior                  = "always"
      + signing_protocol                  = "sigv4"
    }

  # module.terrahouse_aws.aws_s3_bucket_policy.bucket_policy will be created
  + resource "aws_s3_bucket_policy" "bucket_policy" {
      + bucket = "1wakpj698imyssoncxa1hxx9bx74xknf"
      + id     = (known after apply)
      + policy = (known after apply)
    }

Plan: 3 to add, 0 to change, 0 to destroy.

───────────────────────────────────────────────────────────────────────────────

Note: You didn't use the -out option to save this plan, so Terraform can't
guarantee to take exactly these actions if you run "terraform apply" now.
```

I am going to bring this up in office hours.
Found out I was working in main so I used stash and had to do `git add <filename>` for a all the files.

When running `tf destroy` only these are deleted
- module.terrahouse_aws.terraform_data.content_version
- module.terrahouse_aws.aws_s3_object.index_html
- module.terrahouse_aws.aws_s3_object.error_html
- module.terrahouse_aws.aws_s3_bucket_website_configuration.website_configuration
- module.terrahouse_aws.aws_s3_bucket.website_bucket

but not the OAC or Cloudfront distribution so maybe that's the issue
I am going to mannually delete the Cloudfront Distribution and OAC. 

** SOLUTION** I rewatched to video and found a "[" instead of "{" and his error.html did not have the replace_triggered by and the ignore_changes was commented out. 
Seems to work by not changing anything with `tf plan` when changing index.html

### Change Content Version

1. In terraform.tfvars change content_version to equal 2.

2. Do `tf plan` and it should apply changes this time but it doesn't see any changes.

3. The problem is in main.tf, the value of content_version is hardcoded and we need to add var.content_version instead.

4. Do a `tf apply`. It notices the changes in index.html and applies them because we changed our content_version in terraform.tfvars

### Add to documentation and Commit, PR, merge, then tag

1. In week1.md add

```markdown
## Terraform Data

Plain data values such as Local Values and Input Variables don't have any side-effects to plan against and so they aren't valid in replace_triggered_by. You can use terraform_data's behavior of planning an action each time input changes to indirectly use a plain value to trigger replacement.

e.g. 
```go
  lifecycle {
    replace_triggered_by = [terraform_data.content_version.output]
    ignore_changes = [etag]
  }
}
```
[Data](https://developer.hashicorp.com/terraform/language/resources/terraform-data)

```
```

2. Stage the changes and commit with
> #31 only upload index.html is content version has changed

3. Check off issues. Copy issues.

4. Create PR and add issues. Squash and merge

5. Destroy on 31 branch. Tag main with 1.6.0 and close Gitpod

## "Invalidate Cache and Local Exec" Video

### Create an issue

1. In GitHub create an issue with name
> Invalidate Cloudfront Distribution
with a description 
> - [ ] data_source for content_version
> - [ ] trigger cloudfront distribution invalidation

2. Create Branch off of issue and open branch is Gitpod

### Invalidate CloudFront Distribution using Terraform

1. If you had ChatGPT 4, it could write it for you. Basically we are using terraform_data as a resource to do this like we did with content_version. 
Everytime we run a new content version it is going to run a command to invalidate the cache.

Put this in resource-cdn.tf:
```go
esource "terraform_data" "invalidate_cache" {
  triggers_replace = terraform_data.content_version.output

  provisioner "local-exec" {
    # https://developer.hashicorp.com/terraform/language/expressions/strings#heredoc-strings
    command = <<COMMAND
aws cloudfront create-invalidation \
--distribution-id ${aws_cloudfront_distribution.s3_distribution.id} \
--paths '/*'
    COMMAND

  }
}
```
2. Go to heredoc for terraform in browser.
> https://developer.hashicorp.com/terraform/language/expressions/strings#heredoc-strings


3. In week1.md add 

```markdown
## Provisioners

Provisioners allow you to execute commands on compute instances e.g. a AWS CLI command.

They are not recommended for use by Hashicorp because Configuration Management tools such as Ansible are a better fit, but the functionality exists. 

[Provisioners](https://developer.hashicorp.com/terraform/language/resources/provisioners/syntax)

### Local-exec

This will execute a command on the machine running the terraform commands eg. plan and apply

```go
resource "aws_instance" "web" {
  # ...

  provisioner "local-exec" {
    command = "echo The server's IP address is ${self.private_ip}"
  }
}
```
[Local-exec](https://developer.hashicorp.com/terraform/language/resources/provisioners/local-exec)
### Remote-exec

This will execute commands on a machine which you targer. You will need to provide credentials such as ssh to get into the machine

```go
resource "aws_instance" "web" {
  # ...

  # Establishes connection to be used by all
  # generic remote provisioners (i.e. file/remote-exec)
  connection {
    type     = "ssh"
    user     = "root"
    password = var.root_password
    host     = self.public_ip
  }

  provisioner "remote-exec" {
    inline = [
      "puppet apply",
      "consul join ${aws_instance.web.private_ip}",
    ]
  }
}
```
[Remote-exec](https://developer.hashicorp.com/terraform/language/resources/provisioners/remote-exec)
```
```

3. Do `tf init` and `tf plan`. If it asks for content_version, you need to add it to terraform.tfvars. Then `tf apply`
Now it invalidates.

### Add Output for Cloudfront URL

1. In nested outputs.tf add this to output a URL to access our website

```go
output "cloudfront_url" {
  value = aws_cloudfront_distribution.s3_distribution.domain_name
} 
```

2. In outputs.tf reference the "cloudfront_url"
```go
output "cloudfront_url" {
  description = "The CloudFront Distribution Domain Name"
  value = module.terrahouse_aws.cloudfront_url
}
```
3. Do `tf apply` and link for cloudfront_url is provided. See if website is working. 

4. Change some stuff in index.html then do a `tf plan`. It shouldn't notice any changes because we haven't incremented content_version.

5. In terraform.tfvars change content_version to 3. Run `tf plan` and it should see a change for index.html as well as it will perform the invalidate cache. 

6. Do `tf apply`. It won't invalidate that quicky to show when going to website according to video but mine did with the new changes.
If not showing go to AWS Console> Cloudfront> Distribution> Invalidatations to see it's status.

### Commit 

1. Do `tf destroy` and wait until it is down.

2. Commit changes with 
> #33 invalidate cloudfront stack when content version changes

3. Check issues. Copy issues.

4. Create pull request with issues in comment. Squash and Merge.

5. Add tag 1.7.0 to main. 

## "Assets Upload and For Each" video

### Create issue

1. Create a new issue named
> Assets Upload
with description of
> - [ ] Use a For Each to upload assets
> - [ ] Explore different data structures in Terraform
> - [ ] Terraform Console

2. Create a branch from issue and launch Gitpod.

### Create assets folder and Serve on http-server

1. Find an Image from Online that your Terratowns is going to be about. Create an assets directory in Gitpod under publicand drag it over.asse

2. In index.html under the body h1 add the image
`<img src="/assets/NAME OF YOUR PHOTO"`

3. Grab another image of your thing you are doing. Add it to the assets folder.

4. Under the first image in body index.html add the image

5. Add to gitpod.yml to serve our site using http-server
```yml
- name: http-server
    before: |
      npm install --global http-server
    command:
      http-server

```
The website is here
> https://www.npmjs.com/package/http-server

6. Make a new Gitpod terminal tab and run
`npm install --global http-server`
`http-server`

7. Go to the link.

### For Each for Uploading Assets Folder to S3 and TF console

1. In Gitpod Terminal and type
`tf init`
`tf console`

2. Search for terraform fileset in a web browser
> https://developer.hashicorp.com/terraform/language/functions/fileset

3. Still in tf console do these to list out files in /assets
`path.root`
`fileset("${path.root}/public/assets", "*")`

4. We want to specify further for fileset to get pictures
`fileset("${path.root}/public/assets","*.{jpeg,png,gif}")`

5. Copy the last command and paste it in resource-storage.tf under "index_html" block


6. In resource-storage,tf before our fileset, name our "upload_assests" and paste in the fileset into the for_each and uncomment it. Add the bucket, key, source, etag, and lifecycle like we did in the previous objects. 

7. The key is where we want to go to assets directory. Use interporlation "${}" to not hardcode things in to allow more pictures.
`key = "assets/${each.key}`

8. The source is where we get the path of where the files are 
`source = "${path.root}/public/assets/${each.key}"`

9. The etag is just doing filemd5 to hash the source
`etag = filemd5("${path.root}/public/assets/${each.key}")`

In the end it should look like this

```go
resource "aws_s3_object" "upload_assets" {
  for_each = fileset("${path.root}/public/assets","*.{jpeg,png,gif}")
  bucket = aws_s3_bucket.website_bucket.bucket
  key    = "assets/${each.key}"
  source = "${path.root}/public/assets/${each.key}"
   etag = filemd5("${path.root}/public/assets/${each.key}")
  lifecycle {
    replace_triggered_by = [terraform_data.content_version.output]
    ignore_changes = [etag]
  }
}
```

10. Do `tf plan` and we should see how picture files are going to upload.
Do `tf apply --auto-approve` and it should take a while.

11. Take the cloudfront-url and you should see all your pictures on the webpage meaning that the files did upload to the S3 bucket and CloudFront is serving them.

### Add documentation and Create Assets Path for Portability 

1. In week1.md add

```markdown
## For Each Expressions

For each allows us to enumerate over complex data types

```sh
[for s in var.list : upper(s)]
```
This is mostly usedful when you are creating multiples of a cloud resource and you want to reduce the amount of repetitive terraform code.

[For Each Expression](https://developer.hashicorp.com/terraform/language/expressions/for)
```
```

2. Looking back at our resource-storage.tf, we forgot that somethings are hardcoded and if someone wants to name it something other than that, the code will fail. 
E.G. `"${path.root}/public/assets"` in 
`for_each = fileset("${path.root}/public/assets","*.{jpeg,png,gif}")` 

This would be better as a variable instead.
`var.assets_path`

Fix the code.

3. The code should look like this:

```go
resource "aws_s3_object" "upload_assets" {
  for_each = fileset(var.assets_path,"*.{jpeg,png,gif}")
  bucket = aws_s3_bucket.website_bucket.bucket
  key    = "assets/${each.key}"
  source = "${var.assets_path}/${each.key}"
   etag = filemd5("${var.assets_path}/${each.key}")
  lifecycle {
    replace_triggered_by = [terraform_data.content_version.output]
    ignore_changes = [etag]
  }
}
```

4. Define `var.assets_path` in variables.tf nested and reference in variables.tf.

5. Add assets_path to terraform.tfvars and terraform.tfvars. Reset content_version to 1. 

6. Add to main.tf assets_path
Do a `tf plan` to make sure it is working then do `tf destroy`

### Commit, PR, Squash Merge, and Tag

1. Stage and commit all changes with
> #35 upload assets using a for_each

2. Update issue. Copy issue.

3. Create PR, comment issues. Squash and merge

4. Add tags to main 1.8.0 and close Gitpod.

## "Gitgraph" Video

### Create issue
1. In GitHub create a issue named
> Add Git Graph to Gitpod Configuration
with description
> We want to add the git graph extensions in hopes of fixing our git graph issues

2. Create a branch off of issue and open Gitpod

### Add GitGraph to gitpod.yml

1. In Gitpod extensions find the GitGraph extension and click on the clog. Click Copy extension ID

2. In gitpod.yml add the extension ID to extensions
`mhutchie.git-graph`

3. Commit the changes with
> #37 add the known extension to our gitpod configuration task file

4. Make a PR and squash and merge.

5. Tag the main 1.8.1

### Add git-log--graph

1. In Gitpod extensions find git-log--graph and click the cog. Click copy extension ID.

2. In gitpod.yml add the extension ID to extensions
`phil294.git-log--graph`

3. Since our branch is main, we want to add the commit to the previous branch instead.
`git checkout 37-add-git-grapb-to-gitpod-configuration`

4. Commit with 
> #37 add another git graph alternative to ensure we can see graphing

5. Create PR. Squash and merge.
If you didn't do `git pull` on main earlier you it might tell you that it can't merge.
To fix this either merge the branches in Gitpod or Resolve Conflict after creating PR.

6. tag main with 1.8.2 and close gitpod. 
If you are running the validator after this leave gitpod open.

## Validator

1. With GitPod open on your last tag with the 1.8.2, do a `tf apply`